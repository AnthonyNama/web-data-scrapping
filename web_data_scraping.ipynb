{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "web data scraping.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNp6e1IrPkPjsQD2zazmq3Q",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AnthonyNama/web-data-scrapping/blob/master/web_data_scraping.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MwOukyv0wRwN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Y5EPuATW-Aw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import requests"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mKzYS5sdxIVY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rWsMOdJhWrA3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from bs4 import BeautifulSoup"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "caxXPQnt2ZQ4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from IPython.display import HTML"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Owy1mBbgXFk-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BASE_URL = 'https://arxiv.org'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uRhisVG0c5BN",
        "colab_type": "text"
      },
      "source": [
        "**Computer vison and pattern recognition**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uOjHGpKQXabA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "url = 'https://arxiv.org/list/cs.CV/new'\n",
        "html_doc = requests.get(url).text\n",
        "soup = BeautifulSoup(html_doc, \"html.parser\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "34Tal9Pqc1BA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "submissionCV = [div.get_text() for div in soup.find_all('div') if 'class' in div.attrs and div.attrs['class'] == ['list-dateline']][0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t7yqDp1teJR3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "linksCV = [BASE_URL + link.get('href') for link in soup.find_all(\"a\") if 'title' in link.attrs and link.attrs['title'] == 'Abstract']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s8zCs5pjevTV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "titlesCV = [div.get_text()[8:][:-1] for div in soup.find_all(\"div\") if 'class' in div.attrs and div.attrs['class'] == ['list-title', 'mathjax']]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RNET1pdJig18",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "abstractsCV = []\n",
        "for div in soup.find_all(\"div\"):\n",
        "    if 'class' in div.attrs and div.attrs['class'] == ['meta']:\n",
        "        if div.p != None:\n",
        "            abstractsCV.append(div.p.get_text())\n",
        "        else:\n",
        "            abstractsCV.append(\"\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fnlw-7nru186",
        "colab_type": "text"
      },
      "source": [
        "```\n",
        "Filters what I want\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s2wtAIfHuzjv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "my_keywordsCV = [\"GAN\", \"Capsule\"]  # This is an example"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iLwg5edvwKAT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "a3e24526-28c4-4daa-9c60-cad868f8456e"
      },
      "source": [
        "print(submissionCV)"
      ],
      "execution_count": 450,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Submissions received from  Wed  8 Apr 20  to  Thu  9 Apr 20, announced Fri, 10 Apr 20\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KHB2hcZiwNWM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "CV = pd.DataFrame({'Title': titlesCV, 'Abstract': abstractsCV, 'Link': linksCV})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GOv6dcBVyc3j",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "outputId": "7f917d07-569c-4ca6-9771-137e4bfe6c96"
      },
      "source": [
        "CV"
      ],
      "execution_count": 452,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Title</th>\n",
              "      <th>Abstract</th>\n",
              "      <th>Link</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Leveraging 2D Data to Learn Textured 3D Mesh G...</td>\n",
              "      <td>Numerous methods have been proposed for probab...</td>\n",
              "      <td>https://arxiv.org/abs/2004.04180</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>The GeoLifeCLEF 2020 Dataset</td>\n",
              "      <td>Understanding the geographic distribution of s...</td>\n",
              "      <td>https://arxiv.org/abs/2004.04192</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Transferable, Controllable, and Inconspicuous ...</td>\n",
              "      <td>The success of DNNs has driven the extensive a...</td>\n",
              "      <td>https://arxiv.org/abs/2004.04199</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Deep Manifold Prior</td>\n",
              "      <td>We present a prior for manifold structured dat...</td>\n",
              "      <td>https://arxiv.org/abs/2004.04242</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Estimating Grape Yield on the Vine from Multip...</td>\n",
              "      <td>Estimating grape yield prior to harvest is imp...</td>\n",
              "      <td>https://arxiv.org/abs/2004.04278</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>85</th>\n",
              "      <td>A Physics-based Noise Formation Model for Extr...</td>\n",
              "      <td></td>\n",
              "      <td>https://arxiv.org/abs/2003.12751</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>86</th>\n",
              "      <td>How Not to Give a FLOP: Combining Regularizati...</td>\n",
              "      <td></td>\n",
              "      <td>https://arxiv.org/abs/2003.13593</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>87</th>\n",
              "      <td>Manifold-Aware CycleGAN for High Resolution St...</td>\n",
              "      <td></td>\n",
              "      <td>https://arxiv.org/abs/2004.00173</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>88</th>\n",
              "      <td>Evolving Normalization-Activation Layers</td>\n",
              "      <td></td>\n",
              "      <td>https://arxiv.org/abs/2004.02967</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>89</th>\n",
              "      <td>A Polynomial Neural Network with Controllable ...</td>\n",
              "      <td></td>\n",
              "      <td>https://arxiv.org/abs/2004.03955</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>90 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                Title  ...                              Link\n",
              "0   Leveraging 2D Data to Learn Textured 3D Mesh G...  ...  https://arxiv.org/abs/2004.04180\n",
              "1                        The GeoLifeCLEF 2020 Dataset  ...  https://arxiv.org/abs/2004.04192\n",
              "2   Transferable, Controllable, and Inconspicuous ...  ...  https://arxiv.org/abs/2004.04199\n",
              "3                                 Deep Manifold Prior  ...  https://arxiv.org/abs/2004.04242\n",
              "4   Estimating Grape Yield on the Vine from Multip...  ...  https://arxiv.org/abs/2004.04278\n",
              "..                                                ...  ...                               ...\n",
              "85  A Physics-based Noise Formation Model for Extr...  ...  https://arxiv.org/abs/2003.12751\n",
              "86  How Not to Give a FLOP: Combining Regularizati...  ...  https://arxiv.org/abs/2003.13593\n",
              "87  Manifold-Aware CycleGAN for High Resolution St...  ...  https://arxiv.org/abs/2004.00173\n",
              "88           Evolving Normalization-Activation Layers  ...  https://arxiv.org/abs/2004.02967\n",
              "89  A Polynomial Neural Network with Controllable ...  ...  https://arxiv.org/abs/2004.03955\n",
              "\n",
              "[90 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 452
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mum3IxA1yFKv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "CV = pd.DataFrame({'Title': titlesCV, 'Abstract': abstractsCV, 'Link': linksCV})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "flIYBKCF6Py-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "6e284cd5-bfae-41c2-872b-b469c4e04b37"
      },
      "source": [
        "CV.shape"
      ],
      "execution_count": 454,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(90, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 454
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Le1Ejo1V6Z4Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i, row in CV.iterrows():\n",
        "    x = []\n",
        "    for keyword in my_keywordsCV:\n",
        "        x = x + re.findall(keyword, row['Title'] + row['Abstract'])\n",
        "    if not x:\n",
        "        CV.drop([i], axis=0, inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pkr_Hnb_6o-b",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 744
        },
        "outputId": "5e38fdb2-f085-4481-ecff-c1eaca5b87fd"
      },
      "source": [
        "CV.style.format({\"Link\": lambda x: '<a href=\"{}\">{}</a>'.format(x, x)})"
      ],
      "execution_count": 456,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<style  type=\"text/css\" >\n",
              "</style><table id=\"T_fbe2a86a_7c32_11ea_8655_0242ac1c0002\" ><thead>    <tr>        <th class=\"blank level0\" ></th>        <th class=\"col_heading level0 col0\" >Title</th>        <th class=\"col_heading level0 col1\" >Abstract</th>        <th class=\"col_heading level0 col2\" >Link</th>    </tr></thead><tbody>\n",
              "                <tr>\n",
              "                        <th id=\"T_fbe2a86a_7c32_11ea_8655_0242ac1c0002level0_row0\" class=\"row_heading level0 row0\" >10</th>\n",
              "                        <td id=\"T_fbe2a86a_7c32_11ea_8655_0242ac1c0002row0_col0\" class=\"data row0 col0\" >Masked GANs for Unsupervised Depth and Pose Prediction with Scale  Consistency</td>\n",
              "                        <td id=\"T_fbe2a86a_7c32_11ea_8655_0242ac1c0002row0_col1\" class=\"data row0 col1\" >Previous works have shown that adversarial learning can be used for\n",
              "unsupervised monocular depth and visual odometry (VO) estimation. However, the\n",
              "performance of pose and depth networks is limited by occlusions and visual\n",
              "field changes. Because of the incomplete correspondence of visual information\n",
              "between frames caused by motion, target images cannot be synthesized completely\n",
              "from source images via view reconstruction and bilinear interpolation. The\n",
              "reconstruction loss based on the difference between synthesized and real target\n",
              "images will be affected by the incomplete reconstruction. Besides, the data\n",
              "distribution of unreconstructed regions will be learned and help the\n",
              "discriminator distinguish between real and fake images, thereby causing the\n",
              "case that the generator may fail to compete with the discriminator. Therefore,\n",
              "a MaskNet is designed in this paper to predict these regions and reduce their\n",
              "impacts on the reconstruction loss and adversarial loss. The impact of\n",
              "unreconstructed regions on discriminator is tackled by proposing a boolean mask\n",
              "scheme, as shown in Fig. 1. Furthermore, we consider the scale consistency of\n",
              "our pose network by utilizing a new scale-consistency loss, therefore our pose\n",
              "network is capable of providing the full camera trajectory over the long\n",
              "monocular sequence. Extensive experiments on KITTI dataset show that each\n",
              "component proposed in this paper contributes to the performance, and both of\n",
              "our depth and trajectory prediction achieve competitive performance.\n",
              "</td>\n",
              "                        <td id=\"T_fbe2a86a_7c32_11ea_8655_0242ac1c0002row0_col2\" class=\"data row0 col2\" ><a href=\"https://arxiv.org/abs/2004.04345\">https://arxiv.org/abs/2004.04345</a></td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_fbe2a86a_7c32_11ea_8655_0242ac1c0002level0_row1\" class=\"row_heading level0 row1\" >28</th>\n",
              "                        <td id=\"T_fbe2a86a_7c32_11ea_8655_0242ac1c0002row1_col0\" class=\"data row1 col0\" >TuiGAN: Learning Versatile Image-to-Image Translation with Two Unpaired  Images</td>\n",
              "                        <td id=\"T_fbe2a86a_7c32_11ea_8655_0242ac1c0002row1_col1\" class=\"data row1 col1\" >An unsupervised image-to-image translation (UI2I) task deals with learning a\n",
              "mapping between two domains without paired images. While existing UI2I methods\n",
              "usually require numerous unpaired images from different domains for training,\n",
              "there are many scenarios where training data is quite limited. In this paper,\n",
              "we argue that even if each domain contains a single image, UI2I can still be\n",
              "achieved. To this end, we propose TuiGAN, a generative model that is trained on\n",
              "only two unpaired images and amounts to one-shot unsupervised learning. With\n",
              "TuiGAN, an image is translated in a coarse-to-fine manner where the generated\n",
              "image is gradually refined from global structures to local details. We conduct\n",
              "extensive experiments to verify that our versatile method can outperform strong\n",
              "baselines on a wide variety of UI2I tasks. Moreover, TuiGAN is capable of\n",
              "achieving comparable performance with the state-of-the-art UI2I models trained\n",
              "with sufficient data.\n",
              "</td>\n",
              "                        <td id=\"T_fbe2a86a_7c32_11ea_8655_0242ac1c0002row1_col2\" class=\"data row1 col2\" ><a href=\"https://arxiv.org/abs/2004.04634\">https://arxiv.org/abs/2004.04634</a></td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_fbe2a86a_7c32_11ea_8655_0242ac1c0002level0_row2\" class=\"row_heading level0 row2\" >33</th>\n",
              "                        <td id=\"T_fbe2a86a_7c32_11ea_8655_0242ac1c0002row2_col0\" class=\"data row2 col0\" >Inpainting via Generative Adversarial Networks for CMB data analysis</td>\n",
              "                        <td id=\"T_fbe2a86a_7c32_11ea_8655_0242ac1c0002row2_col1\" class=\"data row2 col1\" >In this work, we propose a new method to inpaint the CMB signal in regions\n",
              "masked out following a point source extraction process. We adopt a modified\n",
              "Generative Adversarial Network (GAN) and compare different combinations of\n",
              "internal (hyper-)parameters and training strategies. We study the performance\n",
              "using a suitable $\\mathcal{C}_r$ variable in order to estimate the performance\n",
              "regarding the CMB power spectrum recovery. We consider a test set where one\n",
              "point source is masked out in each sky patch with a 1.83 $\\times$ 1.83 squared\n",
              "degree extension, which, in our gridding, corresponds to 64 $\\times$ 64 pixels.\n",
              "The GAN is optimized for estimating performance on Planck 2018 total intensity\n",
              "simulations. The training makes the GAN effective in reconstructing a masking\n",
              "corresponding to about 1500 pixels with $1\\%$ error down to angular scales\n",
              "corresponding to about 5 arcminutes.\n",
              "</td>\n",
              "                        <td id=\"T_fbe2a86a_7c32_11ea_8655_0242ac1c0002row2_col2\" class=\"data row2 col2\" ><a href=\"https://arxiv.org/abs/2004.04177\">https://arxiv.org/abs/2004.04177</a></td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_fbe2a86a_7c32_11ea_8655_0242ac1c0002level0_row3\" class=\"row_heading level0 row3\" >40</th>\n",
              "                        <td id=\"T_fbe2a86a_7c32_11ea_8655_0242ac1c0002row3_col0\" class=\"data row3 col0\" >Score-Guided Generative Adversarial Networks</td>\n",
              "                        <td id=\"T_fbe2a86a_7c32_11ea_8655_0242ac1c0002row3_col1\" class=\"data row3 col1\" >We propose a Generative Adversarial Network (GAN) that introduces an\n",
              "evaluator module using pre-trained networks. The proposed model, called\n",
              "score-guided GAN (ScoreGAN), is trained with an evaluation metric for GANs,\n",
              "i.e., the Inception score, as a rough guide for the training of the generator.\n",
              "By using another pre-trained network instead of the Inception network, ScoreGAN\n",
              "circumvents the overfitting of the Inception network in order that generated\n",
              "samples do not correspond to adversarial examples of the Inception network.\n",
              "Also, to prevent the overfitting, the evaluation metrics are employed only as\n",
              "an auxiliary role, while the conventional target of GANs is mainly used.\n",
              "Evaluated with the CIFAR-10 dataset, ScoreGAN demonstrated an Inception score\n",
              "of 10.36$\\pm$0.15, which corresponds to state-of-the-art performance.\n",
              "Furthermore, to generalize the effectiveness of ScoreGAN, the model was further\n",
              "evaluated with another dataset, i.e., the CIFAR-100; as a result, ScoreGAN\n",
              "outperformed the other existing methods, where the Fr\\'echet Inception Distance\n",
              "(FID) of ScoreGAN trained over the CIFAR-100 dataset was 13.98.\n",
              "</td>\n",
              "                        <td id=\"T_fbe2a86a_7c32_11ea_8655_0242ac1c0002row3_col2\" class=\"data row3 col2\" ><a href=\"https://arxiv.org/abs/2004.04396\">https://arxiv.org/abs/2004.04396</a></td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_fbe2a86a_7c32_11ea_8655_0242ac1c0002level0_row4\" class=\"row_heading level0 row4\" >43</th>\n",
              "                        <td id=\"T_fbe2a86a_7c32_11ea_8655_0242ac1c0002row4_col0\" class=\"data row4 col0\" >Adversarial Latent Autoencoders</td>\n",
              "                        <td id=\"T_fbe2a86a_7c32_11ea_8655_0242ac1c0002row4_col1\" class=\"data row4 col1\" >Autoencoder networks are unsupervised approaches aiming at combining\n",
              "generative and representational properties by learning simultaneously an\n",
              "encoder-generator map. Although studied extensively, the issues of whether they\n",
              "have the same generative power of GANs, or learn disentangled representations,\n",
              "have not been fully addressed. We introduce an autoencoder that tackles these\n",
              "issues jointly, which we call Adversarial Latent Autoencoder (ALAE). It is a\n",
              "general architecture that can leverage recent improvements on GAN training\n",
              "procedures. We designed two autoencoders: one based on a MLP encoder, and\n",
              "another based on a StyleGAN generator, which we call StyleALAE. We verify the\n",
              "disentanglement properties of both architectures. We show that StyleALAE can\n",
              "not only generate 1024x1024 face images with comparable quality of StyleGAN,\n",
              "but at the same resolution can also produce face reconstructions and\n",
              "manipulations based on real images. This makes ALAE the first autoencoder able\n",
              "to compare with, and go beyond the capabilities of a generator-only type of\n",
              "architecture.\n",
              "</td>\n",
              "                        <td id=\"T_fbe2a86a_7c32_11ea_8655_0242ac1c0002row4_col2\" class=\"data row4 col2\" ><a href=\"https://arxiv.org/abs/2004.04467\">https://arxiv.org/abs/2004.04467</a></td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_fbe2a86a_7c32_11ea_8655_0242ac1c0002level0_row5\" class=\"row_heading level0 row5\" >87</th>\n",
              "                        <td id=\"T_fbe2a86a_7c32_11ea_8655_0242ac1c0002row5_col0\" class=\"data row5 col0\" >Manifold-Aware CycleGAN for High Resolution Structural-to-DTI Synthesis</td>\n",
              "                        <td id=\"T_fbe2a86a_7c32_11ea_8655_0242ac1c0002row5_col1\" class=\"data row5 col1\" ></td>\n",
              "                        <td id=\"T_fbe2a86a_7c32_11ea_8655_0242ac1c0002row5_col2\" class=\"data row5 col2\" ><a href=\"https://arxiv.org/abs/2004.00173\">https://arxiv.org/abs/2004.00173</a></td>\n",
              "            </tr>\n",
              "    </tbody></table>"
            ],
            "text/plain": [
              "<pandas.io.formats.style.Styler at 0x7fd7d7bcb208>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 456
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1EFgw82TntN5",
        "colab_type": "text"
      },
      "source": [
        "**Machine learning**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SJSulkVUdEdM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "url = 'https://arxiv.org/list/cs.LG/new'\n",
        "html_doc = requests.get(url).text\n",
        "soup = BeautifulSoup(html_doc, \"html.parser\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_vfwvJpXrgCP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "submissionLG = [div.get_text() for div in soup.find_all('div') if 'class' in div.attrs and div.attrs['class'] == ['list-dateline']][0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JygGKffBru_k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "linksLG = [BASE_URL + link.get('href') for link in soup.find_all(\"a\") if 'title' in link.attrs and link.attrs['title'] == 'Abstract']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tJS3eQ5Dry1r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "titlesLG = [div.get_text()[8:][:-1] for div in soup.find_all(\"div\") if 'class' in div.attrs and div.attrs['class'] == ['list-title', 'mathjax']]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YnjOe3vOr8iV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "abstractsLG = []\n",
        "for div in soup.find_all(\"div\"):\n",
        "    if 'class' in div.attrs and div.attrs['class'] == ['meta']:\n",
        "        if div.p != None:\n",
        "            abstractsLG.append(div.p.get_text())\n",
        "        else:\n",
        "            abstractsLG.append(\"\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IUNtpdDlu9z8",
        "colab_type": "text"
      },
      "source": [
        "```\n",
        "Filters what I want\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P3R-tQc6sbMy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "my_keywordsLG = ['natural language processing', 'text mining', \"NLP\", \"question answering\", \"spectral methods\", \"SVD\"]  # This is an example"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K2RaXmoxwEHP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "c23ac9a1-3a60-4410-d10a-69c05ec9b5e8"
      },
      "source": [
        "print(submissionLG)"
      ],
      "execution_count": 463,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Submissions received from  Wed  8 Apr 20  to  Thu  9 Apr 20, announced Fri, 10 Apr 20\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ab2H1-bPzDI3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "LG = pd.DataFrame({'Title': titlesLG, 'Abstract': abstractsLG, 'Link': linksLG})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "13oHNvbj2nWh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#LG.style.format({\"Link\": lambda x: '<a href=\"{}\">{}</a>'.format(x, x)})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mi6oqcu611k_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "85b38670-70f3-4f09-a3b4-3a06adaf1c29"
      },
      "source": [
        "LG.shape"
      ],
      "execution_count": 466,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(130, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 466
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xTKQAAmqzK7c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i, row in LG.iterrows():\n",
        "    x = []\n",
        "    for keyword in my_keywordsLG:\n",
        "        x = x + re.findall(keyword, row['Title'] + row['Abstract'])\n",
        "    if not x:\n",
        "        LG.drop([i], axis=0, inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lSYlPaMw1lKp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 303
        },
        "outputId": "0bda231e-391b-4df3-8d8a-2bb0196ba985"
      },
      "source": [
        "LG.style.format({\"Link\": lambda x: '<a href=\"{}\">{}</a>'.format(x, x)})"
      ],
      "execution_count": 468,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<style  type=\"text/css\" >\n",
              "</style><table id=\"T_fceb705c_7c32_11ea_8655_0242ac1c0002\" ><thead>    <tr>        <th class=\"blank level0\" ></th>        <th class=\"col_heading level0 col0\" >Title</th>        <th class=\"col_heading level0 col1\" >Abstract</th>        <th class=\"col_heading level0 col2\" >Link</th>    </tr></thead><tbody>\n",
              "                <tr>\n",
              "                        <th id=\"T_fceb705c_7c32_11ea_8655_0242ac1c0002level0_row0\" class=\"row_heading level0 row0\" >54</th>\n",
              "                        <td id=\"T_fceb705c_7c32_11ea_8655_0242ac1c0002row0_col0\" class=\"data row0 col0\" >Calibrating Structured Output Predictors for Natural Language Processing</td>\n",
              "                        <td id=\"T_fceb705c_7c32_11ea_8655_0242ac1c0002row0_col1\" class=\"data row0 col1\" >We address the problem of calibrating prediction confidence for output\n",
              "entities of interest in natural language processing (NLP) applications. It is\n",
              "important that NLP applications such as named entity recognition and question\n",
              "answering produce calibrated confidence scores for their predictions,\n",
              "especially if the system is to be deployed in a safety-critical domain such as\n",
              "healthcare. However, the output space of such structured prediction models is\n",
              "often too large to adapt binary or multi-class calibration methods directly. In\n",
              "this study, we propose a general calibration scheme for output entities of\n",
              "interest in neural-network based structured prediction models. Our proposed\n",
              "method can be used with any binary class calibration scheme and a neural\n",
              "network model. Additionally, we show that our calibration method can also be\n",
              "used as an uncertainty-aware, entity-specific decoding step to improve the\n",
              "performance of the underlying model at no additional training cost or data\n",
              "requirements. We show that our method outperforms current calibration\n",
              "techniques for named-entity-recognition, part-of-speech and question answering.\n",
              "We also improve our model's performance from our decoding step across several\n",
              "tasks and benchmark datasets. Our method improves the calibration and model\n",
              "performance on out-of-domain test scenarios as well.\n",
              "</td>\n",
              "                        <td id=\"T_fceb705c_7c32_11ea_8655_0242ac1c0002row0_col2\" class=\"data row0 col2\" ><a href=\"https://arxiv.org/abs/2004.04361\">https://arxiv.org/abs/2004.04361</a></td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_fceb705c_7c32_11ea_8655_0242ac1c0002level0_row1\" class=\"row_heading level0 row1\" >128</th>\n",
              "                        <td id=\"T_fceb705c_7c32_11ea_8655_0242ac1c0002row1_col0\" class=\"data row1 col0\" >Towards Faithfully Interpretable NLP Systems: How should we define and  evaluate faithfulness?</td>\n",
              "                        <td id=\"T_fceb705c_7c32_11ea_8655_0242ac1c0002row1_col1\" class=\"data row1 col1\" ></td>\n",
              "                        <td id=\"T_fceb705c_7c32_11ea_8655_0242ac1c0002row1_col2\" class=\"data row1 col2\" ><a href=\"https://arxiv.org/abs/2004.03685\">https://arxiv.org/abs/2004.03685</a></td>\n",
              "            </tr>\n",
              "    </tbody></table>"
            ],
            "text/plain": [
              "<pandas.io.formats.style.Styler at 0x7fd7d7ab56d8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 468
        }
      ]
    }
  ]
}